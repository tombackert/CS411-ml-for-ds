{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tombackert/CS411-ml-for-ds/blob/main/MNIST_NN_tensorflow_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSV_OlCFKOD"
      },
      "source": [
        "# Training a neural network on MNIST with Keras\n",
        "\n",
        "This simple example demonstrates how to plug TensorFlow Datasets (TFDS) into a Keras model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8y9ZkLXmAZc"
      },
      "source": [
        "Copyright 2020 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGw9EgE0tC0C"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/datasets/keras_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTBSvHcSLBzc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjI6VgOBf0v0"
      },
      "source": [
        "## Step 1: Create your input pipeline\n",
        "\n",
        "Start by building an efficient input pipeline using advices from:\n",
        "* The [Performance tips](https://www.tensorflow.org/datasets/performances) guide\n",
        "* The [Better performance with the `tf.data` API](https://www.tensorflow.org/guide/data_performance#optimize_performance) guide\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3aH3vP_XLI8"
      },
      "source": [
        "### Load a dataset\n",
        "\n",
        "Load the MNIST dataset with the following arguments:\n",
        "\n",
        "* `shuffle_files=True`: The MNIST data is only stored in a single file, but for larger datasets with multiple files on disk, it's good practice to shuffle them when training.\n",
        "* `as_supervised=True`: Returns a tuple `(img, label)` instead of a dictionary `{'image': img, 'label': label}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUMhCXhFXdHQ"
      },
      "outputs": [],
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgwCFAcWXQTx"
      },
      "source": [
        "### Build a training pipeline\n",
        "\n",
        "Apply the following transformations:\n",
        "\n",
        "* `tf.data.Dataset.map`: TFDS provide images of type `tf.uint8`, while the model expects `tf.float32`. Therefore, you need to normalize images.\n",
        "* `tf.data.Dataset.cache` As you fit the dataset in memory, cache it before shuffling for a better performance.<br/>\n",
        "__Note:__ Random transformations should be applied after caching.\n",
        "* `tf.data.Dataset.shuffle`: For true randomness, set the shuffle buffer to the full dataset size.<br/>\n",
        "__Note:__ For large datasets that can't fit in memory, use `buffer_size=1000` if your system allows it.\n",
        "* `tf.data.Dataset.batch`: Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
        "* `tf.data.Dataset.prefetch`: It is good practice to end the pipeline by prefetching [for performance](https://www.tensorflow.org/guide/data_performance#prefetching)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haykx2K9XgiI"
      },
      "outputs": [],
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(128)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbsMy4X1XVFv"
      },
      "source": [
        "### Build an evaluation pipeline\n",
        "\n",
        "Your testing pipeline is similar to the training pipeline with small differences:\n",
        "\n",
        " * You don't need to call `tf.data.Dataset.shuffle`.\n",
        " * Caching is done after batching because batches can be the same between epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0KjuDf7XiqY"
      },
      "outputs": [],
      "source": [
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTFoji3INMEM"
      },
      "source": [
        "## Step 2: Create and train the model\n",
        "\n",
        "Plug the TFDS input pipeline into a simple Keras model, compile the model, and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWqxdmS1NLKA"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),  # 784\n",
        "  tf.keras.layers.Dense(128, activation='relu'),  # 128\n",
        "  tf.keras.layers.Dense(64, activation='relu'),   # 64\n",
        "  tf.keras.layers.Dense(10)                       # 10\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=20,\n",
        "    validation_data=ds_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 1])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "L7t5QVwJ92UR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Diffrent Model Testing"
      ],
      "metadata": {
        "id": "iQYZ8BPChIF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Automate testing of diffrent models\n",
        "\n",
        "def create_model(hidden_layers, neurons_per_layer):\n",
        "    \"\"\"\n",
        "    Function for creating new model with num of hidden layers and nerons per layer as parameter\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "    for neurons in neurons_per_layer:\n",
        "        model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    return model\n",
        "\n",
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    Function for compiling model\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "def plot_metrics(history, model_number):\n",
        "    \"\"\"\n",
        "    Function for plotting metrics of model\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    # Accuracy\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Modell {model_number}: Genauigkeit')\n",
        "    plt.xlabel('Epoche')\n",
        "    plt.ylabel('Genauigkeit')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    # Loss\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Modell {model_number}: Verlust')\n",
        "    plt.xlabel('Epoche')\n",
        "    plt.ylabel('Verlust')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wxSU277mDDTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating all three models\n",
        "\n",
        "model1 = create_model(hidden_layers=1, neurons_per_layer=[128])\n",
        "model2 = create_model(hidden_layers=2, neurons_per_layer=[128, 64])\n",
        "model3 = create_model(hidden_layers=3, neurons_per_layer=[256, 128, 64])"
      ],
      "metadata": {
        "id": "HTRrEL8MDHy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling all three models\n",
        "\n",
        "compile_model(model1)\n",
        "compile_model(model2)\n",
        "compile_model(model3)"
      ],
      "metadata": {
        "id": "Ew2AZATxDLVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training all three models\n",
        "\n",
        "# Model 1\n",
        "start_time1 = time.time()\n",
        "history1 = model1.fit(\n",
        "    ds_train,\n",
        "    epochs=20,\n",
        "    validation_data=ds_test,\n",
        ")\n",
        "end_time1 = time.time()\n",
        "training_time1 = end_time1 - start_time1\n",
        "print(training_time1)\n",
        "\n",
        "# Model 2\n",
        "start_time2 = time.time()\n",
        "history2 = model2.fit(\n",
        "    ds_train,\n",
        "    epochs=20,\n",
        "    validation_data=ds_test,\n",
        ")\n",
        "end_time2 = time.time()\n",
        "training_time2 = end_time2 - start_time2\n",
        "print(training_time2)\n",
        "\n",
        "# Model 3\n",
        "start_time3 = time.time()\n",
        "history3 = model3.fit(\n",
        "    ds_train,\n",
        "    epochs=20,\n",
        "    validation_data=ds_test,\n",
        ")\n",
        "end_time3 = time.time()\n",
        "training_time3 = end_time3 - start_time3\n",
        "print(training_time3)"
      ],
      "metadata": {
        "id": "8ESlQwX3EDke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting loss\n",
        "plot_metrics(history1, 1)\n",
        "plot_metrics(history2, 2)\n",
        "plot_metrics(history3, 3)"
      ],
      "metadata": {
        "id": "DYd6R36WEThk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def evaluate_model(model, ds_test, time):\n",
        "    test_loss, test_accuracy = model.evaluate(ds_test, verbose=0)\n",
        "    return test_loss, test_accuracy, time"
      ],
      "metadata": {
        "id": "ZEbdCuzwEdJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1, acc1, time1 = evaluate_model(model1, ds_test, training_time1)\n",
        "loss2, acc2, time2 = evaluate_model(model2, ds_test, training_time2)\n",
        "loss3, acc3, time3 = evaluate_model(model3, ds_test, training_time3)"
      ],
      "metadata": {
        "id": "hPmfIjIcEeUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_time1)"
      ],
      "metadata": {
        "id": "4qRu2ZxaYxuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'Modell': ['Modell 1', 'Modell 2', 'Modell 3'],\n",
        "    'Accuracy': [acc1, acc2, acc3],\n",
        "    'Loss': [loss1, loss2, loss3],\n",
        "    'Training Time (s)': [time1, time2, time3]\n",
        "})\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "3R02OCaxEeTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYaFimO7bGHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Hyperparameter Testing"
      ],
      "metadata": {
        "id": "DPuO8Pb_hAfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(learning_rate, batch_size, epochs):\n",
        "    # Create a new instance of the model\n",
        "    model = create_model(hidden_layers=2, neurons_per_layer=[128, 64])\n",
        "\n",
        "\n",
        "    # Compile the model with the specified learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['sparse_categorical_accuracy'],\n",
        "    )\n",
        "\n",
        "    # Unbatch and re-batch the data to change the batch size\n",
        "    ds_train_batched = ds_train.unbatch().batch(batch_size)\n",
        "    ds_test_batched = ds_test.unbatch().batch(batch_size)\n",
        "\n",
        "    # Record the training time\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        ds_train_batched,\n",
        "        epochs=epochs,\n",
        "        validation_data=ds_test_batched,\n",
        "        verbose=1\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_accuracy = model.evaluate(ds_test_batched, verbose=0)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': epochs,\n",
        "        'training_time': training_time,\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "F_cqZYpFbYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lr_0_1 = train_model(learning_rate=0.1, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "7BFUIu1Qbu3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lr_0_01 = train_model(learning_rate=0.01, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "fTfVrT0WbwEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_lr_0_001 = train_model(learning_rate=0.001, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "79jW2T_LbxMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the results\n",
        "lr_results = pd.DataFrame({\n",
        "    'Learning Rate': [result_lr_0_1['learning_rate'], result_lr_0_01['learning_rate'], result_lr_0_001['learning_rate']],\n",
        "    'Test Accuracy': [result_lr_0_1['test_accuracy'], result_lr_0_01['test_accuracy'], result_lr_0_001['test_accuracy']],\n",
        "    'Test Loss': [result_lr_0_1['test_loss'], result_lr_0_01['test_loss'], result_lr_0_001['test_loss']],\n",
        "    'Training Time (s)': [result_lr_0_1['training_time'], result_lr_0_01['training_time'], result_lr_0_001['training_time']]\n",
        "})\n",
        "\n",
        "print(lr_results)"
      ],
      "metadata": {
        "id": "XrfAoiDXezmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(history, title):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each learning rate\n",
        "plot_learning_curve(result_lr_0_1['history'], 'Learning Rate = 0.1')\n",
        "plot_learning_curve(result_lr_0_01['history'], 'Learning Rate = 0.01')\n",
        "plot_learning_curve(result_lr_0_001['history'], 'Learning Rate = 0.001')"
      ],
      "metadata": {
        "id": "JKwu2Kuve26M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs = 10\n",
        "result_epochs_10 = train_model(learning_rate=0.01, batch_size=128, epochs=10)\n",
        "\n",
        "# Epochs = 20\n",
        "result_epochs_20 = train_model(learning_rate=0.01, batch_size=128, epochs=20)"
      ],
      "metadata": {
        "id": "fPWliirSe6y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Size = 64\n",
        "result_batch_64 = train_model(learning_rate=0.01, batch_size=64, epochs=10)\n",
        "\n",
        "# Batch Size = 128\n",
        "result_batch_128 = train_model(learning_rate=0.01, batch_size=128, epochs=10)"
      ],
      "metadata": {
        "id": "oSqWMY2de8-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_results = pd.DataFrame({\n",
        "    'Epochs': [result_epochs_10['epochs'], result_epochs_20['epochs']],\n",
        "    'Test Accuracy': [result_epochs_10['test_accuracy'], result_epochs_20['test_accuracy']],\n",
        "    'Test Loss': [result_epochs_10['test_loss'], result_epochs_20['test_loss']],\n",
        "    'Training Time (s)': [result_epochs_10['training_time'], result_epochs_20['training_time']]\n",
        "})\n",
        "\n",
        "print(epoch_results)"
      ],
      "metadata": {
        "id": "nil8i6gQe_OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_results = pd.DataFrame({\n",
        "    'Batch Size': [result_batch_64['batch_size'], result_batch_128['batch_size']],\n",
        "    'Test Accuracy': [result_batch_64['test_accuracy'], result_batch_128['test_accuracy']],\n",
        "    'Test Loss': [result_batch_64['test_loss'], result_batch_128['test_loss']],\n",
        "    'Training Time (s)': [result_batch_64['training_time'], result_batch_128['training_time']]\n",
        "})\n",
        "\n",
        "print(batch_results)"
      ],
      "metadata": {
        "id": "pV88w4Y1fAUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curve(result_epochs_10['history'], 'Epochs = 10')\n",
        "plot_learning_curve(result_epochs_20['history'], 'Epochs = 20')"
      ],
      "metadata": {
        "id": "CvUb7c3YfBwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curve(result_batch_64['history'], 'Batch Size = 64')\n",
        "plot_learning_curve(result_batch_128['history'], 'Batch Size = 128')"
      ],
      "metadata": {
        "id": "P2U_M6HWfCzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4: Building a NN for a different Dataset\n"
      ],
      "metadata": {
        "id": "9AbWcK22gyie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "J3JcBKI6hcSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "8XLiXuc3hh_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)\n",
        "print(\"Test data shape:\", X_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "oGWm5Zx0hkDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "cMmqp2UahoMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "def create_cifar_model():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(32, 32, 3)))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "nvur1rAfhx59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_cifar_model()\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "yEE3B10vh0em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "N2Txyz4Ah270"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record training time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time"
      ],
      "metadata": {
        "id": "2geh7rrRh4SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "ydHuDcsCh6cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curves(history)"
      ],
      "metadata": {
        "id": "hp8yhDfnh8kO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}